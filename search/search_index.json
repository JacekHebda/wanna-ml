{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WANNA # WANNA is a ML platform product that integrates security, data governance and deployment best practices with well-defined constructs provided by GCP Vertex AI native services to make research move faster, more efficiently and predictively from ML prototypes to production models whilst leaving behind the limitations of an on-prem infrastructure. Objective # Embrace the advantages and scalability of a ML Platform as a Service and adopt Vertex AI premise of \u201cone AI platform, every ML tool you need\u201d. Having the goal for a unified set of APIs, frameworks, UIs and secure data access patterns from a central point will increase productivity across the Research and Data Science teams whilst removing operational costs. Proposal # Vertex AI native services are vast and welcomed, however we must have clear communication, interfaces, requirements and prioritization to cover all researchers and ML Engineers use cases within this ecosystem. WANNA will provide this thought its CLI, standardized project structure, CI/CD integration and addressing and supporting customer needs. Problem definition # Remove barriers and standardize the management of ML projects on Vertex AI through the lenses of unified tooling.","title":"Overview"},{"location":"#welcome-to-wanna","text":"WANNA is a ML platform product that integrates security, data governance and deployment best practices with well-defined constructs provided by GCP Vertex AI native services to make research move faster, more efficiently and predictively from ML prototypes to production models whilst leaving behind the limitations of an on-prem infrastructure.","title":"Welcome to WANNA"},{"location":"#objective","text":"Embrace the advantages and scalability of a ML Platform as a Service and adopt Vertex AI premise of \u201cone AI platform, every ML tool you need\u201d. Having the goal for a unified set of APIs, frameworks, UIs and secure data access patterns from a central point will increase productivity across the Research and Data Science teams whilst removing operational costs.","title":"Objective"},{"location":"#proposal","text":"Vertex AI native services are vast and welcomed, however we must have clear communication, interfaces, requirements and prioritization to cover all researchers and ML Engineers use cases within this ecosystem. WANNA will provide this thought its CLI, standardized project structure, CI/CD integration and addressing and supporting customer needs.","title":"Proposal"},{"location":"#problem-definition","text":"Remove barriers and standardize the management of ML projects on Vertex AI through the lenses of unified tooling.","title":"Problem definition"},{"location":"installation/","text":"Installation # Requirements # To run wanna-ml, you need docker daemon, a Python >=3.7 environment setup and gcloud cli installed Installing with Pipx # The recommended way to install wanna-ml is to use pipx. pipx will install the package in isolation so you won\u2019t have conflicts with other packages in your environment. You can install wanna-ml like this: pipx install wanna-ml You can upgrade the package like this: pipx upgrade wanna-ml if you want to manage the project with poetry pipx install poetry Installing with Pip # wanna-ml is a normal Python package and you can install it with pip: pip install wanna-ml Be aware, that installing it globally might cause conflicts with other installed package. You can solve this problem by using a pipenv environment: pipenv local 3.8.12 pip install wanna-ml You will need to add wanna-ml to your PATH.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"To run wanna-ml, you need docker daemon, a Python >=3.7 environment setup and gcloud cli installed","title":"Requirements"},{"location":"installation/#installing-with-pipx","text":"The recommended way to install wanna-ml is to use pipx. pipx will install the package in isolation so you won\u2019t have conflicts with other packages in your environment. You can install wanna-ml like this: pipx install wanna-ml You can upgrade the package like this: pipx upgrade wanna-ml if you want to manage the project with poetry pipx install poetry","title":"Installing with Pipx"},{"location":"installation/#installing-with-pip","text":"wanna-ml is a normal Python package and you can install it with pip: pip install wanna-ml Be aware, that installing it globally might cause conflicts with other installed package. You can solve this problem by using a pipenv environment: pipenv local 3.8.12 pip install wanna-ml You will need to add wanna-ml to your PATH.","title":"Installing with Pip"},{"location":"cli/commands/","text":"CLI Reference # This page provides documentation for wanna command-line tool. wanna # Usage: [OPTIONS] COMMAND [ARGS]... Options: --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. components # Usage: components [OPTIONS] COMMAND [ARGS]... create # Usage: components create [OPTIONS] Options: --output-dir PATH The output directory where wanna-ml repository will be created [required] init # Usage: init [OPTIONS] Options: --output-dir PATH The output directory where wanna-ml repository will be created [required] -t, --template [sklearn|blank] Choose from available repository templates [default: sklearn] job # Usage: job [OPTIONS] COMMAND [ARGS]... build # Usage: job build [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to build. Choose 'all' to build all jobs. [default: all] push # Usage: job push [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -v, --version TEXT Job version [required] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to push. Choose 'all' to push all jobs. [default: all] -m, --mode [all|manifests|containers|quick] Pipeline push mode, due to CI/CD not allowing to push to docker registry from GCP Agent, we need to split it. Use all for dev [default: PushMode.all] report # Displays a link to the cost report per wanna_project and optionally per job name Usage: job report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to report. Choose 'all' to report all jobs. [default: all] run # Usage: job run [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -v, --version TEXT Job version [required] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to run. Choose 'all' to run all jobs. [default: all] -p, --hp-params PATH Path to the params file in yaml format -s, --sync Runs the job in sync mode run-manifest # Usage: job run-manifest [OPTIONS] Options: -v, --manifest TEXT Job deployment manifest -p, --hp-params PATH Path to the params file in yaml format -s, --sync Runs the pipeline in sync mode stop # Usage: job stop [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to stop. Choose 'all' to stop all jobs. [default: all] managed-notebook # Usage: managed-notebook [OPTIONS] COMMAND [ARGS]... create # Create a Managed Workbench Notebook. If there already is a notebook with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one. When the notebook instance is created, you will be given a URL link to JupyterLab. Usage: managed-notebook create [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to create. Choose 'all' to create all managed_notebooks. [default: all] delete # Delete a Managed Workbench Notebook. Usage: managed-notebook delete [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to delete. Choose 'all' to delete all managed_notebooks. [default: all] report # Displays a link to the cost report per wanna_project and optionally per instance name Usage: managed-notebook report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to report. Choose 'all' to report all managed_notebooks. [default: all] sync # Synchronize existing Managed Notebooks with wanna.yaml Reads current notebooks where label is defined per field wanna_project.name in wanna.yaml Does a diff between what is on GCP and what is on yaml Create the ones defined in yaml and missing in GCP Delete the ones in GCP that are not in wanna.yaml Usage: managed-notebook sync [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] notebook # Usage: notebook [OPTIONS] COMMAND [ARGS]... create # Create a User-Managed Workbench Notebook. If there already is a notebook with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one. When the notebook instance is created, you will be given a URL link to JupyterLab. Usage: notebook create [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one notebook from your wanna-ml yaml configuration to create. Choose 'all' to create all notebooks. [default: all] -o, --owner TEXT delete # Delete a User-Managed Workbench Notebook. Usage: notebook delete [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one notebook from your wanna-ml yaml configuration to delete. Choose 'all' to delete all notebooks. [default: all] report # Displays a link to the cost report per wanna_project and optionally per instance name Usage: notebook report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one notebook from your wanna-ml yaml configuration to report. Choose 'all' to report all notebooks. [default: all] ssh # SSH connect to the Compute Engine instance that is behind the Jupyter Notebook. This will only work if the notebook is already running. Please note that you can connect to only one instance with one command call. If you have more notebooks defined in your YAML config, you have to select to which you want to connect to, instance_name \"all\" will be refused. Usage: notebook ssh [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify to which notebook you want to connect via ssh. Selecting 'all' will work only if there is just one notebook defined in your configuration, an error will be thrown otherwise. [default: all] -b, --background / -i, --interactive Interactive mode will start a bash directly in the Compute Engine instance backing the Jupyter notebook. Background mode serves more like a port-forwarding, you will be able to connect to the Jupyter Lab at localhost:{LOCAL_PORT} [default: interactive] --port INTEGER Jupyter Lab will be accessible at this port at localhost. [default: 8080] pipeline # Usage: pipeline [OPTIONS] COMMAND [ARGS]... build # Usage: pipeline build [OPTIONS] Options: -v, --version TEXT Pipeline version [default: dev] -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to compile. Choose 'all' to compile all pipelines. [default: all] -m, --mode [all|manifests|containers|quick] Pipeline push mode, due to CI/CD not allowing to push to docker registry from GCP Agent, we need to split it. Use all for dev [default: PushMode.all] deploy # Usage: pipeline deploy [OPTIONS] Options: -v, --version TEXT Pipeline version [required] -e, --env TEXT Pipeline env [default: local] -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to deploy. Choose 'all' to deploy all pipelines. [default: all] push # Usage: pipeline push [OPTIONS] Options: -v, --version TEXT Pipeline version [required] -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to push. Choose 'all' to push all pipelines. [default: all] -m, --mode [all|manifests|containers|quick] Pipeline push mode, due to CI/CD not allowing to push to docker registry from GCP Agent, we need to split it. Use all for dev [default: PushMode.all] report # Displays a link to the cost report per wanna_project and optionally per instance name Usage: pipeline report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to report. Choose 'all' to report all managed_notebooks. [default: all] run # Usage: pipeline run [OPTIONS] Options: -v, --version TEXT Pipeline version [default: dev] -p, --params PATH Path to the params file in yaml format [default: params.yaml] -s, --sync Runs the pipeline in sync mode -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to run. Choose 'all' to run all pipelines. [default: all] run-manifest # Usage: pipeline run-manifest [OPTIONS] Options: -v, --manifest TEXT Job deployment manifest -p, --params PATH Path to the params file in yaml format [default: params.yaml] -s, --sync Runs the pipeline in sync mode tensorboard # Usage: tensorboard [OPTIONS] COMMAND [ARGS]... create # Create Tensorboard Instance in GCP Vertex AI Experiments. If there already is a tensorboard with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one. When the tensorboard instance is created, you will be given a full resource name. Usage: tensorboard create [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one tensorboard from your wanna-ml yaml configuration to create. Choose 'all' to create all tensorboards. [default: all] delete # Delete Tensorboard Instance in GCP Vertex AI Experiments. Usage: tensorboard delete [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one tensorboard from your wanna-ml yaml configuration to delete. Choose 'all' to delete all tensorboards. [default: all] list # List Tensorboard Instances in GCP Vertex AI Experiments. We also show Tensorboard Experiments and Tensorboard Runs for each Instance in the tree format. Usage: tensorboard list [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] --region TEXT Overwrites the region from wanna-ml yaml configuration --filter TEXT GCP filter expression for tensorboard instances. Read more on GCP filters on https://cloud.google.com/sdk/gclo ud/reference/topic/filters Example: display_name=my- tensorboard. Example: labels.wanna_project:* - to show all tensorboard created by wanna-ml. Example: labels.wanna_project:sushi-ssl. --url / --no-url Weather to show URL link to experiments [default: url] version # Usage: version [OPTIONS]","title":"Plugins"},{"location":"cli/commands/#cli-reference","text":"This page provides documentation for wanna command-line tool.","title":"CLI Reference"},{"location":"cli/commands/#wanna","text":"Usage: [OPTIONS] COMMAND [ARGS]... Options: --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation.","title":"wanna"},{"location":"cli/commands/#components","text":"Usage: components [OPTIONS] COMMAND [ARGS]...","title":"components"},{"location":"cli/commands/#create","text":"Usage: components create [OPTIONS] Options: --output-dir PATH The output directory where wanna-ml repository will be created [required]","title":"create"},{"location":"cli/commands/#init","text":"Usage: init [OPTIONS] Options: --output-dir PATH The output directory where wanna-ml repository will be created [required] -t, --template [sklearn|blank] Choose from available repository templates [default: sklearn]","title":"init"},{"location":"cli/commands/#job","text":"Usage: job [OPTIONS] COMMAND [ARGS]...","title":"job"},{"location":"cli/commands/#build","text":"Usage: job build [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to build. Choose 'all' to build all jobs. [default: all]","title":"build"},{"location":"cli/commands/#push","text":"Usage: job push [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -v, --version TEXT Job version [required] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to push. Choose 'all' to push all jobs. [default: all] -m, --mode [all|manifests|containers|quick] Pipeline push mode, due to CI/CD not allowing to push to docker registry from GCP Agent, we need to split it. Use all for dev [default: PushMode.all]","title":"push"},{"location":"cli/commands/#report","text":"Displays a link to the cost report per wanna_project and optionally per job name Usage: job report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to report. Choose 'all' to report all jobs. [default: all]","title":"report"},{"location":"cli/commands/#run","text":"Usage: job run [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -v, --version TEXT Job version [required] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to run. Choose 'all' to run all jobs. [default: all] -p, --hp-params PATH Path to the params file in yaml format -s, --sync Runs the job in sync mode","title":"run"},{"location":"cli/commands/#run-manifest","text":"Usage: job run-manifest [OPTIONS] Options: -v, --manifest TEXT Job deployment manifest -p, --hp-params PATH Path to the params file in yaml format -s, --sync Runs the pipeline in sync mode","title":"run-manifest"},{"location":"cli/commands/#stop","text":"Usage: job stop [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one job from your wanna-ml yaml configuration to stop. Choose 'all' to stop all jobs. [default: all]","title":"stop"},{"location":"cli/commands/#managed-notebook","text":"Usage: managed-notebook [OPTIONS] COMMAND [ARGS]...","title":"managed-notebook"},{"location":"cli/commands/#create_1","text":"Create a Managed Workbench Notebook. If there already is a notebook with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one. When the notebook instance is created, you will be given a URL link to JupyterLab. Usage: managed-notebook create [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to create. Choose 'all' to create all managed_notebooks. [default: all]","title":"create"},{"location":"cli/commands/#delete","text":"Delete a Managed Workbench Notebook. Usage: managed-notebook delete [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to delete. Choose 'all' to delete all managed_notebooks. [default: all]","title":"delete"},{"location":"cli/commands/#report_1","text":"Displays a link to the cost report per wanna_project and optionally per instance name Usage: managed-notebook report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to report. Choose 'all' to report all managed_notebooks. [default: all]","title":"report"},{"location":"cli/commands/#sync","text":"Synchronize existing Managed Notebooks with wanna.yaml Reads current notebooks where label is defined per field wanna_project.name in wanna.yaml Does a diff between what is on GCP and what is on yaml Create the ones defined in yaml and missing in GCP Delete the ones in GCP that are not in wanna.yaml Usage: managed-notebook sync [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default]","title":"sync"},{"location":"cli/commands/#notebook","text":"Usage: notebook [OPTIONS] COMMAND [ARGS]...","title":"notebook"},{"location":"cli/commands/#create_2","text":"Create a User-Managed Workbench Notebook. If there already is a notebook with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one. When the notebook instance is created, you will be given a URL link to JupyterLab. Usage: notebook create [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one notebook from your wanna-ml yaml configuration to create. Choose 'all' to create all notebooks. [default: all] -o, --owner TEXT","title":"create"},{"location":"cli/commands/#delete_1","text":"Delete a User-Managed Workbench Notebook. Usage: notebook delete [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one notebook from your wanna-ml yaml configuration to delete. Choose 'all' to delete all notebooks. [default: all]","title":"delete"},{"location":"cli/commands/#report_2","text":"Displays a link to the cost report per wanna_project and optionally per instance name Usage: notebook report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one notebook from your wanna-ml yaml configuration to report. Choose 'all' to report all notebooks. [default: all]","title":"report"},{"location":"cli/commands/#ssh","text":"SSH connect to the Compute Engine instance that is behind the Jupyter Notebook. This will only work if the notebook is already running. Please note that you can connect to only one instance with one command call. If you have more notebooks defined in your YAML config, you have to select to which you want to connect to, instance_name \"all\" will be refused. Usage: notebook ssh [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify to which notebook you want to connect via ssh. Selecting 'all' will work only if there is just one notebook defined in your configuration, an error will be thrown otherwise. [default: all] -b, --background / -i, --interactive Interactive mode will start a bash directly in the Compute Engine instance backing the Jupyter notebook. Background mode serves more like a port-forwarding, you will be able to connect to the Jupyter Lab at localhost:{LOCAL_PORT} [default: interactive] --port INTEGER Jupyter Lab will be accessible at this port at localhost. [default: 8080]","title":"ssh"},{"location":"cli/commands/#pipeline","text":"Usage: pipeline [OPTIONS] COMMAND [ARGS]...","title":"pipeline"},{"location":"cli/commands/#build_1","text":"Usage: pipeline build [OPTIONS] Options: -v, --version TEXT Pipeline version [default: dev] -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to compile. Choose 'all' to compile all pipelines. [default: all] -m, --mode [all|manifests|containers|quick] Pipeline push mode, due to CI/CD not allowing to push to docker registry from GCP Agent, we need to split it. Use all for dev [default: PushMode.all]","title":"build"},{"location":"cli/commands/#deploy","text":"Usage: pipeline deploy [OPTIONS] Options: -v, --version TEXT Pipeline version [required] -e, --env TEXT Pipeline env [default: local] -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to deploy. Choose 'all' to deploy all pipelines. [default: all]","title":"deploy"},{"location":"cli/commands/#push_1","text":"Usage: pipeline push [OPTIONS] Options: -v, --version TEXT Pipeline version [required] -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to push. Choose 'all' to push all pipelines. [default: all] -m, --mode [all|manifests|containers|quick] Pipeline push mode, due to CI/CD not allowing to push to docker registry from GCP Agent, we need to split it. Use all for dev [default: PushMode.all]","title":"push"},{"location":"cli/commands/#report_3","text":"Displays a link to the cost report per wanna_project and optionally per instance name Usage: pipeline report [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one managed_notebook from your wanna-ml yaml configuration to report. Choose 'all' to report all managed_notebooks. [default: all]","title":"report"},{"location":"cli/commands/#run_1","text":"Usage: pipeline run [OPTIONS] Options: -v, --version TEXT Pipeline version [default: dev] -p, --params PATH Path to the params file in yaml format [default: params.yaml] -s, --sync Runs the pipeline in sync mode -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one pipeline from your wanna-ml yaml configuration to run. Choose 'all' to run all pipelines. [default: all]","title":"run"},{"location":"cli/commands/#run-manifest_1","text":"Usage: pipeline run-manifest [OPTIONS] Options: -v, --manifest TEXT Job deployment manifest -p, --params PATH Path to the params file in yaml format [default: params.yaml] -s, --sync Runs the pipeline in sync mode","title":"run-manifest"},{"location":"cli/commands/#tensorboard","text":"Usage: tensorboard [OPTIONS] COMMAND [ARGS]...","title":"tensorboard"},{"location":"cli/commands/#create_3","text":"Create Tensorboard Instance in GCP Vertex AI Experiments. If there already is a tensorboard with the same name in the same location and project, you will be prompt if you want to delete the existing one and start a new one. When the tensorboard instance is created, you will be given a full resource name. Usage: tensorboard create [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one tensorboard from your wanna-ml yaml configuration to create. Choose 'all' to create all tensorboards. [default: all]","title":"create"},{"location":"cli/commands/#delete_2","text":"Delete Tensorboard Instance in GCP Vertex AI Experiments. Usage: tensorboard delete [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] -n, --name TEXT Specify only one tensorboard from your wanna-ml yaml configuration to delete. Choose 'all' to delete all tensorboards. [default: all]","title":"delete"},{"location":"cli/commands/#list","text":"List Tensorboard Instances in GCP Vertex AI Experiments. We also show Tensorboard Experiments and Tensorboard Runs for each Instance in the tree format. Usage: tensorboard list [OPTIONS] Options: -f, --file PATH Path to the wanna-ml yaml configuration [env var: WANNA_FILE; default: wanna.yaml] -p, --profile TEXT Name of the GCP profile you want to use. Profiles are loaded from wanna-ml yaml config and (optionally) from WANNA_GCP_PROFILE_PATH [env var: WANNA_GCP_PROFILE_NAME; default: default] --region TEXT Overwrites the region from wanna-ml yaml configuration --filter TEXT GCP filter expression for tensorboard instances. Read more on GCP filters on https://cloud.google.com/sdk/gclo ud/reference/topic/filters Example: display_name=my- tensorboard. Example: labels.wanna_project:* - to show all tensorboard created by wanna-ml. Example: labels.wanna_project:sushi-ssl. --url / --no-url Weather to show URL link to experiments [default: url]","title":"list"},{"location":"cli/commands/#version","text":"Usage: version [OPTIONS]","title":"version"},{"location":"tutorial/","text":"WANNA - Get started # Installation # Install using pip install -U wanna-ml . For more information on the installation process and requirements, visit out installation page in documentation Authentication # WANNA-ML relies on gcloud for user authentication. Install the gcloud CLI - follow official guide Authenticate with the gcloud init Set you Google Application Credentials gcloud auth application-default login Docker Build # You can use a local Docker daemon to build Docker images, but it is not required. You are free to choose between local building on GCP Cloud Build. If you prefer local Docker image building, install Docker Desktop . GCP IAM Roles and Permissions # Different WANNA-ML calls require different GCP permissions to create given resources on GCP. Our documentation page lists recommended GCP IAM roles for each wanna command.","title":"Get Started"},{"location":"tutorial/#wanna-get-started","text":"","title":"WANNA - Get started"},{"location":"tutorial/#installation","text":"Install using pip install -U wanna-ml . For more information on the installation process and requirements, visit out installation page in documentation","title":"Installation"},{"location":"tutorial/#authentication","text":"WANNA-ML relies on gcloud for user authentication. Install the gcloud CLI - follow official guide Authenticate with the gcloud init Set you Google Application Credentials gcloud auth application-default login","title":"Authentication"},{"location":"tutorial/#docker-build","text":"You can use a local Docker daemon to build Docker images, but it is not required. You are free to choose between local building on GCP Cloud Build. If you prefer local Docker image building, install Docker Desktop .","title":"Docker Build"},{"location":"tutorial/#gcp-iam-roles-and-permissions","text":"Different WANNA-ML calls require different GCP permissions to create given resources on GCP. Our documentation page lists recommended GCP IAM roles for each wanna command.","title":"GCP IAM Roles and Permissions"},{"location":"tutorial/docker/","text":"WANNA Docker # Multiple resources created by WANNA rely on Docker containers. We make it easy for you to build your images either locally or using GCP Cloud Build. However, the GCP Cloud Build will not be allowed to use in production. Types of docker images # We currently support three types of docker images: provided_image - you supply a link to the docker image in the registry. We don't build anything, just redirect this link to GCP. local_build_image - you supply a Dockerfile with a context directory and additional information. We build the image for you on your machine or in the cloud. notebook_ready_image - you supply a list of pip requirements to install in your Jupyter Notebook. This is useful if you want to start a notebook with custom libraries, but you don't want to handle Dockerfile information. Referencing docker images # Each docker image must have a name . By this name, you can later reference it in resource configuration, usually as docker_image_ref . Example: docker: images: - build_type: local_build_image name: custom-notebook-container-julia context_dir: . dockerfile: Dockerfile.notebook repository: wanna-samples cloud_build: true notebooks: - name: wanna-notebook-julia environment: docker_image_ref: custom-notebook-container-julia Local build vs GCP Cloud Build # By default, all docker images are built locally on your machine and then pushed to the registry. For faster testing lifecycle you can build images directly using GCP Cloud Build. The only needed change is to set cloud_build: true in docker section of the WANNA yaml config or set WANNA_DOCKER_BUILD_IN_CLOUD=true (env variable takes precedence). Building in the cloud is generally faster as the docker images are automatically already in the registry and there is no need to push the images over the network. That makes it suitable for fast testing. However, building images in the cloud is not allowed for production. Build configuration # When building locally, we offer you a way to set additional build parameters. These parameters must be specified in a separate yaml file in path WANNA_DOCKER_BUILD_CONFIG . If this is not set, it defaults to the dockerbuild.yaml in the working directory. You can set: build_args: Dict[str, str] labels: Dict[str, str] network: Optional[str] platforms: Optional[List[str]] secrets: Union[str, List[str]] ssh: Optional[str] target: Optional[str] These parameters refer to standard docker build parameters . One example use case can be when you want to git clone your internal repository during the docker build. In the dockerbuild.yaml : ssh: github=~/.ssh/id_rsa In the Dockerfile : RUN mkdir -m 700 /root/.ssh; \\ touch -m 600 /root/.ssh/known_hosts; \\ ssh-keyscan git.int.avast.com > /root/.ssh/known_hosts RUN --mount=type=ssh,id=github git clone git@git.your.company.com:your_profile/your_repo.git Parameters for docker section # docker section takes following parameters: - images - list of docker images, see below - repository - GCP Artifact Registry repository for pushing images - registry - (optional) GCP Artifact Registry, when not set it defaults to {gcp_profile.region}-docker.pkg.dev - cloud_build - false (default) to build locally, true to use GCP Cloud Build Provided image parameters: # build_type: provided_image name - this will later be used in docker_image_ref in other resources image_url - link to the image Local build image parameters: # build_type: local_build_image name - this will later be used in docker_image_ref in other resources build_args - (optional) docker build args context_dir - Path to the docker build context directory dockerfile - Path to the Dockerfile Notebook ready image: # build_type: notebook_ready_image name - this will later be used in docker_image_ref in other resources build_args - (optional) docker build args base_image - (optional) base notebook docker image, you can check available images here when not set, it defaults to standard base CPU notebook. requirements_txt - Path to the requirements.txt file Roles and permissions # Permission and suggested roles (applying the principle of least privilege) required for docker images manipulation: WANNA action Permissions Suggested Roles build in Cloud Build cloudbuild.builds.create and more roles/cloudbuild.builds.builder push artifactregistry.repositories.uploadArtifacts , artifactregistry.tags.create , artifactregistry.tags.update roles/artifactregistry.writer For building the docker images locally, you will need permission to push to GCP as described above and running local Docker daemon. You also have to authenticate docker with GCP, detailed documentation is here . But generally, you should be fine with running: gcloud auth login gcloud auth configure-docker europe-west1-docker.pkg.dev # Add more comma-separated repository hostnames if you wish Full list of available roles and permission.","title":"WANNA Docker"},{"location":"tutorial/docker/#wanna-docker","text":"Multiple resources created by WANNA rely on Docker containers. We make it easy for you to build your images either locally or using GCP Cloud Build. However, the GCP Cloud Build will not be allowed to use in production.","title":"WANNA Docker"},{"location":"tutorial/docker/#types-of-docker-images","text":"We currently support three types of docker images: provided_image - you supply a link to the docker image in the registry. We don't build anything, just redirect this link to GCP. local_build_image - you supply a Dockerfile with a context directory and additional information. We build the image for you on your machine or in the cloud. notebook_ready_image - you supply a list of pip requirements to install in your Jupyter Notebook. This is useful if you want to start a notebook with custom libraries, but you don't want to handle Dockerfile information.","title":"Types of docker images"},{"location":"tutorial/docker/#referencing-docker-images","text":"Each docker image must have a name . By this name, you can later reference it in resource configuration, usually as docker_image_ref . Example: docker: images: - build_type: local_build_image name: custom-notebook-container-julia context_dir: . dockerfile: Dockerfile.notebook repository: wanna-samples cloud_build: true notebooks: - name: wanna-notebook-julia environment: docker_image_ref: custom-notebook-container-julia","title":"Referencing docker images"},{"location":"tutorial/docker/#local-build-vs-gcp-cloud-build","text":"By default, all docker images are built locally on your machine and then pushed to the registry. For faster testing lifecycle you can build images directly using GCP Cloud Build. The only needed change is to set cloud_build: true in docker section of the WANNA yaml config or set WANNA_DOCKER_BUILD_IN_CLOUD=true (env variable takes precedence). Building in the cloud is generally faster as the docker images are automatically already in the registry and there is no need to push the images over the network. That makes it suitable for fast testing. However, building images in the cloud is not allowed for production.","title":"Local build vs GCP Cloud Build"},{"location":"tutorial/docker/#build-configuration","text":"When building locally, we offer you a way to set additional build parameters. These parameters must be specified in a separate yaml file in path WANNA_DOCKER_BUILD_CONFIG . If this is not set, it defaults to the dockerbuild.yaml in the working directory. You can set: build_args: Dict[str, str] labels: Dict[str, str] network: Optional[str] platforms: Optional[List[str]] secrets: Union[str, List[str]] ssh: Optional[str] target: Optional[str] These parameters refer to standard docker build parameters . One example use case can be when you want to git clone your internal repository during the docker build. In the dockerbuild.yaml : ssh: github=~/.ssh/id_rsa In the Dockerfile : RUN mkdir -m 700 /root/.ssh; \\ touch -m 600 /root/.ssh/known_hosts; \\ ssh-keyscan git.int.avast.com > /root/.ssh/known_hosts RUN --mount=type=ssh,id=github git clone git@git.your.company.com:your_profile/your_repo.git","title":"Build configuration"},{"location":"tutorial/docker/#parameters-for-docker-section","text":"docker section takes following parameters: - images - list of docker images, see below - repository - GCP Artifact Registry repository for pushing images - registry - (optional) GCP Artifact Registry, when not set it defaults to {gcp_profile.region}-docker.pkg.dev - cloud_build - false (default) to build locally, true to use GCP Cloud Build","title":"Parameters for docker section"},{"location":"tutorial/docker/#provided-image-parameters","text":"build_type: provided_image name - this will later be used in docker_image_ref in other resources image_url - link to the image","title":"Provided image parameters:"},{"location":"tutorial/docker/#local-build-image-parameters","text":"build_type: local_build_image name - this will later be used in docker_image_ref in other resources build_args - (optional) docker build args context_dir - Path to the docker build context directory dockerfile - Path to the Dockerfile","title":"Local build image parameters:"},{"location":"tutorial/docker/#notebook-ready-image","text":"build_type: notebook_ready_image name - this will later be used in docker_image_ref in other resources build_args - (optional) docker build args base_image - (optional) base notebook docker image, you can check available images here when not set, it defaults to standard base CPU notebook. requirements_txt - Path to the requirements.txt file","title":"Notebook ready image:"},{"location":"tutorial/docker/#roles-and-permissions","text":"Permission and suggested roles (applying the principle of least privilege) required for docker images manipulation: WANNA action Permissions Suggested Roles build in Cloud Build cloudbuild.builds.create and more roles/cloudbuild.builds.builder push artifactregistry.repositories.uploadArtifacts , artifactregistry.tags.create , artifactregistry.tags.update roles/artifactregistry.writer For building the docker images locally, you will need permission to push to GCP as described above and running local Docker daemon. You also have to authenticate docker with GCP, detailed documentation is here . But generally, you should be fine with running: gcloud auth login gcloud auth configure-docker europe-west1-docker.pkg.dev # Add more comma-separated repository hostnames if you wish Full list of available roles and permission.","title":"Roles and permissions"},{"location":"tutorial/job/","text":"WANNA Job # Hyper-parameter tuning # A custom job can be simply converted to a hyper-parameter tuning job just by adding one extra parameter called hp_tuning . This will start a series of jobs (instead of just one job) and try to find the best combination of hyper-parameters in regard to a target variable that you specify. Read the official documentation for more information. In general, you have to set which hyper-parameters are changeable, which metric you want to optimize over and how many trials you want to run. You also need to adjust your training script so it would accept hyper-parameters as script arguments and report the optimized metric back to Vertex-Ai. Setting hyper-parameter space # Your code should accept a script arguments with name matching wanna.yaml config. For example, if you want to fine-tune the learning rate in your model: In wanna.yaml config: hp_tuning: parameters: - var_name: learning_rate type: double min: 0.001 max: 1 scale: log And the python script should accept the same argument with the same type: parser = argparse.ArgumentParser() parser.add_argument( '--learning_rate', required=True, type=float, help='learning rate') Currently, you can use parameters of type double , integer , discrete and categorical . Each of them must be specified by var_name , type and additionaly: double : min , max and scale ( linear / log ) integer : min , max and scale ( linear / log ) discrete : values (list of possible values) and scale ( linear / log ) categorical : values (list of possible values) Setting target metric # You can choose to either maximize or minimize your optimized metric. Example in wanna.yaml : hp_tuning: metrics: {'accuracy':'maximize'} parameters: ... Your python script must report back the metric during training. In TensorFlow/Keras you can use a callback to write the metric to the TensorFlow summary - documentation . In any other case, you should use cloudml-hypertune library. import hypertune hpt = hypertune.HyperTune() hpt.report_hyperparameter_tuning_metric( hyperparameter_metric_tag='accuracy', metric_value=0.987, global_step=1000) Setting number of trials and search algorithm # The number of trials can be influenced by max_trial_count and parallel_trial_count . Search through hyper-parameter space can be grid , random or if not any of those two are set, the default Bayesian Optimization will be used.","title":"WANNA Job"},{"location":"tutorial/job/#wanna-job","text":"","title":"WANNA Job"},{"location":"tutorial/job/#hyper-parameter-tuning","text":"A custom job can be simply converted to a hyper-parameter tuning job just by adding one extra parameter called hp_tuning . This will start a series of jobs (instead of just one job) and try to find the best combination of hyper-parameters in regard to a target variable that you specify. Read the official documentation for more information. In general, you have to set which hyper-parameters are changeable, which metric you want to optimize over and how many trials you want to run. You also need to adjust your training script so it would accept hyper-parameters as script arguments and report the optimized metric back to Vertex-Ai.","title":"Hyper-parameter tuning"},{"location":"tutorial/job/#setting-hyper-parameter-space","text":"Your code should accept a script arguments with name matching wanna.yaml config. For example, if you want to fine-tune the learning rate in your model: In wanna.yaml config: hp_tuning: parameters: - var_name: learning_rate type: double min: 0.001 max: 1 scale: log And the python script should accept the same argument with the same type: parser = argparse.ArgumentParser() parser.add_argument( '--learning_rate', required=True, type=float, help='learning rate') Currently, you can use parameters of type double , integer , discrete and categorical . Each of them must be specified by var_name , type and additionaly: double : min , max and scale ( linear / log ) integer : min , max and scale ( linear / log ) discrete : values (list of possible values) and scale ( linear / log ) categorical : values (list of possible values)","title":"Setting hyper-parameter space"},{"location":"tutorial/job/#setting-target-metric","text":"You can choose to either maximize or minimize your optimized metric. Example in wanna.yaml : hp_tuning: metrics: {'accuracy':'maximize'} parameters: ... Your python script must report back the metric during training. In TensorFlow/Keras you can use a callback to write the metric to the TensorFlow summary - documentation . In any other case, you should use cloudml-hypertune library. import hypertune hpt = hypertune.HyperTune() hpt.report_hyperparameter_tuning_metric( hyperparameter_metric_tag='accuracy', metric_value=0.987, global_step=1000)","title":"Setting target metric"},{"location":"tutorial/job/#setting-number-of-trials-and-search-algorithm","text":"The number of trials can be influenced by max_trial_count and parallel_trial_count . Search through hyper-parameter space can be grid , random or if not any of those two are set, the default Bayesian Optimization will be used.","title":"Setting number of trials and search algorithm"},{"location":"tutorial/managed-notebook/","text":"WANNA Managed notebook # It offers a simple way of deploying Jupyter Notebooks on GCP, with minimum environment set up, automatic mount of all GCS buckets in the project, connection do Dataproc clusters and more. Obligatory fields # name - Custom name for this instance owner - Currently only the owner will be able to access the notebook Dataproc clusters and metastore # If you want to run Spark jobs on a Dataproc cluster and also have a Hive Metastore service available as your default Spark SQL engine: Create a Dataproc Metastore in your GCP project & region in the Google Cloud UI Create a Dataproc cluster connected to this metastore, with a subnet specified. e.g.: gcloud dataproc clusters create cluster-test --enable-component-gateway --region europe-west1 --subnet cloud-lab --zone europe-west1-b --single-node --optional-components JUPYTER --dataproc-metastore projects/cloud-lab-304213/locations/europe-west1/services/jacek-test Run your managed notebook. As kernel use Pyspark on the remote Dataproc cluster that you have just created Test your spark Session for access. This example creates a database in your metastore: spark = SparkSession \\ .builder \\ .appName(\"MetastoreTest\") \\ .getOrCreate() query = \"\"\"CREATE DATABASE testdb\"\"\" spark.sql(query) Tensorboard integration # tb-gcp-uploader is needed to upload the logs to the tensorboard instance. A detailed tutorial on this tool can be found here . If you set the tensorboard_ref in the WANNA yaml config, we will export the tensorboard resource name as AIP_TENSORBOARD_LOG_DIR . Additional notebook parameters # Apart from the above, we offer additional parameters for you: machine_type - GCP Compute Engine machine type tags - GCP Compute Engine tags to add to the runtime labels - Custom labels to apply to this instance metadata - Custom metadata to apply to this instance gpu - The hardware GPU accelerator used on this instance. data_disk - Data disk configuration to attach to this instance. network - Currently unavailable subnet - Currently unavailable kernels - Custom kernels given as links to container registry idle_shutdown - True or false idle_shutdown_timeout - Time in minutes, between 10 and 1440 Connecting with VSCode # Install Jupyter Extension Create a new file with the type Jupyter notebook Select the Jupyter Server: local button in the global Status bar or run the Jupyter: Specify local or remote Jupyter server for connections command from the Command Palette (\u21e7\u2318P). Select option Existing URL and input http://localhost:8080 You should be connected. If you get an error saying something like '_xsrf' argument missing from POST. , it is because the VSCode cannot start a python kernel in GCP. The current workaround is to manually start a kernel at http://localhost:8080 and then in the VSCode connect to the exiting kernel in the right upper corner. A more detailed guide on setting a connection with VSCode to Jupyter can be found at https://code.visualstudio.com/docs/datascience/jupyter-notebooks. Example # managed-notebooks: - name: example owner: jacek.hebda@avast.com machine_type: n1-standard-1 labels: notebook_usecase: wanna-notebook-sample tags: metadata: gpu: count: 1 accelerator_type: NVIDIA_TESLA_T4 data_disk: disk_type: pd_standard size_gb: 100 tensorboard_ref: kernels: network: subnet: idle_shutdown: True idle_shutdown_timeout: 180","title":"WANNA Managed Notebook"},{"location":"tutorial/managed-notebook/#wanna-managed-notebook","text":"It offers a simple way of deploying Jupyter Notebooks on GCP, with minimum environment set up, automatic mount of all GCS buckets in the project, connection do Dataproc clusters and more.","title":"WANNA Managed notebook"},{"location":"tutorial/managed-notebook/#obligatory-fields","text":"name - Custom name for this instance owner - Currently only the owner will be able to access the notebook","title":"Obligatory fields"},{"location":"tutorial/managed-notebook/#dataproc-clusters-and-metastore","text":"If you want to run Spark jobs on a Dataproc cluster and also have a Hive Metastore service available as your default Spark SQL engine: Create a Dataproc Metastore in your GCP project & region in the Google Cloud UI Create a Dataproc cluster connected to this metastore, with a subnet specified. e.g.: gcloud dataproc clusters create cluster-test --enable-component-gateway --region europe-west1 --subnet cloud-lab --zone europe-west1-b --single-node --optional-components JUPYTER --dataproc-metastore projects/cloud-lab-304213/locations/europe-west1/services/jacek-test Run your managed notebook. As kernel use Pyspark on the remote Dataproc cluster that you have just created Test your spark Session for access. This example creates a database in your metastore: spark = SparkSession \\ .builder \\ .appName(\"MetastoreTest\") \\ .getOrCreate() query = \"\"\"CREATE DATABASE testdb\"\"\" spark.sql(query)","title":"Dataproc clusters and metastore"},{"location":"tutorial/managed-notebook/#tensorboard-integration","text":"tb-gcp-uploader is needed to upload the logs to the tensorboard instance. A detailed tutorial on this tool can be found here . If you set the tensorboard_ref in the WANNA yaml config, we will export the tensorboard resource name as AIP_TENSORBOARD_LOG_DIR .","title":"Tensorboard integration"},{"location":"tutorial/managed-notebook/#additional-notebook-parameters","text":"Apart from the above, we offer additional parameters for you: machine_type - GCP Compute Engine machine type tags - GCP Compute Engine tags to add to the runtime labels - Custom labels to apply to this instance metadata - Custom metadata to apply to this instance gpu - The hardware GPU accelerator used on this instance. data_disk - Data disk configuration to attach to this instance. network - Currently unavailable subnet - Currently unavailable kernels - Custom kernels given as links to container registry idle_shutdown - True or false idle_shutdown_timeout - Time in minutes, between 10 and 1440","title":"Additional notebook parameters"},{"location":"tutorial/managed-notebook/#connecting-with-vscode","text":"Install Jupyter Extension Create a new file with the type Jupyter notebook Select the Jupyter Server: local button in the global Status bar or run the Jupyter: Specify local or remote Jupyter server for connections command from the Command Palette (\u21e7\u2318P). Select option Existing URL and input http://localhost:8080 You should be connected. If you get an error saying something like '_xsrf' argument missing from POST. , it is because the VSCode cannot start a python kernel in GCP. The current workaround is to manually start a kernel at http://localhost:8080 and then in the VSCode connect to the exiting kernel in the right upper corner. A more detailed guide on setting a connection with VSCode to Jupyter can be found at https://code.visualstudio.com/docs/datascience/jupyter-notebooks.","title":"Connecting with VSCode"},{"location":"tutorial/managed-notebook/#example","text":"managed-notebooks: - name: example owner: jacek.hebda@avast.com machine_type: n1-standard-1 labels: notebook_usecase: wanna-notebook-sample tags: metadata: gpu: count: 1 accelerator_type: NVIDIA_TESLA_T4 data_disk: disk_type: pd_standard size_gb: 100 tensorboard_ref: kernels: network: subnet: idle_shutdown: True idle_shutdown_timeout: 180","title":"Example"},{"location":"tutorial/notebook/","text":"WANNA Notebook # We offer a simple way of managing Jupyter Notebooks on GCP, with multiple ways to set your environment, mount a GCS bucket, and more. Notebook Environments # There are two distinct possibilities for your environment. Use a custom docker image, we recommend you build on top of GCP notebook ready images, either with using one of their images as a base or by using the notebook_ready_image docker type. It is also possible to build your image from scratch, but please follow GCP's recommended principles and port settings as described here . docker: images: - build_type: local_build_image name: custom-notebook-container context_dir: . dockerfile: Dockerfile.notebook repository: wanna-samples cloud_build: true notebooks: - name: wanna-notebook-custom-container environment: docker_image_ref: custom-notebook-container Use a virtual machine image with preconfigured python libraries or TensorFlow / PyTorch / R and more. A complete list of available images can be found here . notebooks: - name: wanna-notebook-vm machine_type: n1-standard-4 environment: vm_image: framework: pytorch version: 1-9-xla os: debian-10 Mounting buckets # We can automatically mount GCS buckets with gcsfuse during the notebook startup. Example: bucket_mounts: - bucket_name: us-burger-gcp-poc-mooncloud bucket_dir: data local_path: /home/jupyter/mounted/gcs Tensorboard integration # tb-gcp-uploader is needed to upload the logs to the tensorboard instance. A detailed tutorial on this tool can be found here . If you set the tensorboard_ref in the WANNA yaml config, we will export the tensorboard resource name as AIP_TENSORBOARD_LOG_DIR . Additional notebook parameters # Apart from setting your computing environment, tensorboard, and bucket mounts, we offer additional parameters for you: zone - GCP location zone machine_type - GCP Compute Engine machine type tags - GCP Compute Engine tags to add to the runtime metadata - Custom metadata to apply to this instance service_account - The email address of a service account to use when running the execution instance_owner - Currently supports one owner only. If not specified, all of the service account users of your VM instance\u2019s service account can use the instance. If specified, only the owner will be able to access the notebook. gpu - The hardware GPU accelerator used on this instance. boot_disk - Boot disk configuration to attach to this instance. data_disk - Data disk configuration to attach to this instance. network - The name of the VPC that this instance is in. Roles and permissions # Permission and suggested roles (applying the principle of least privilege) required for notebook manipulation: WANNA action Permissions Suggested Roles create See full list roles/notebooks.runner , roles/notebooks.admin delete see full list roles/notebooks.admin For accessing the JupyterLab web interface, you must grant the user access to the service account used by the notebooks instance. If the instance owner is set, only this user can access the web interface. Full list of available roles and permission. Local development and SSH # If you wish to develop code in your local IDE and run it on Vertex-AI notebooks, we have a solution for you. Assuming your notebook is already running, you can set up an SSH connection via: wanna notebook ssh --background -n notebook_name Wanna will create an SSH tunnel using GCP IAP from your local environment to your notebook. The --background/-b flag means that the tunnel will be created in the background and you can access the notebook running in GCP at localhost:8080 (port can be customized with --port ). The second possibility is to use --interactive/-i and that will start a bash inside the Compute Engine instance backing your Vertex-AI notebook. Once you set an --background connection to the notebook, you can use your favorite IDE to develop in the notebook. Here we share instructions on how to use VSCode for this. Connecting with VSCode # Install Jupyter Extension Create a new file with the type Jupyter notebook Select the Jupyter Server: local button in the global Status bar or run the Jupyter: Specify local or remote Jupyter server for connections command from the Command Palette (\u21e7\u2318P). Select option Existing URL and input http://localhost:8080 You should be connected. If you get an error saying something like '_xsrf' argument missing from POST. , it is because the VSCode cannot start a python kernel in GCP. The current workaround is to manually start a kernel at http://localhost:8080 and then in the VSCode connect to the exiting kernel in the right upper corner. A more detailed guide on setting a connection with VSCode to Jupyter can be found at https://code.visualstudio.com/docs/datascience/jupyter-notebooks. Example # notebooks: - name: wanna-notebook-trial service_account: instance_owner: machine_type: n1-standard-4 labels: notebook_usecase: wanna-notebook-sample-simple-pip environment: vm_image: framework: pytorch version: 1-9-xla os: debian-10 gpu: count: 1 accelerator_type: NVIDIA_TESLA_V100 install_gpu_driver: true boot_disk: disk_type: pd_standard size_gb: 100 data_disk: disk_type: pd_standard size_gb: 100 bucket_mounts: - bucket_name: us-burger-gcp-poc-mooncloud bucket_dir: data local_path: /home/jupyter/mounted/gcs tensorboard_ref: my-super-tensorboard","title":"WANNA Notebook"},{"location":"tutorial/notebook/#wanna-notebook","text":"We offer a simple way of managing Jupyter Notebooks on GCP, with multiple ways to set your environment, mount a GCS bucket, and more.","title":"WANNA Notebook"},{"location":"tutorial/notebook/#notebook-environments","text":"There are two distinct possibilities for your environment. Use a custom docker image, we recommend you build on top of GCP notebook ready images, either with using one of their images as a base or by using the notebook_ready_image docker type. It is also possible to build your image from scratch, but please follow GCP's recommended principles and port settings as described here . docker: images: - build_type: local_build_image name: custom-notebook-container context_dir: . dockerfile: Dockerfile.notebook repository: wanna-samples cloud_build: true notebooks: - name: wanna-notebook-custom-container environment: docker_image_ref: custom-notebook-container Use a virtual machine image with preconfigured python libraries or TensorFlow / PyTorch / R and more. A complete list of available images can be found here . notebooks: - name: wanna-notebook-vm machine_type: n1-standard-4 environment: vm_image: framework: pytorch version: 1-9-xla os: debian-10","title":"Notebook Environments"},{"location":"tutorial/notebook/#mounting-buckets","text":"We can automatically mount GCS buckets with gcsfuse during the notebook startup. Example: bucket_mounts: - bucket_name: us-burger-gcp-poc-mooncloud bucket_dir: data local_path: /home/jupyter/mounted/gcs","title":"Mounting buckets"},{"location":"tutorial/notebook/#tensorboard-integration","text":"tb-gcp-uploader is needed to upload the logs to the tensorboard instance. A detailed tutorial on this tool can be found here . If you set the tensorboard_ref in the WANNA yaml config, we will export the tensorboard resource name as AIP_TENSORBOARD_LOG_DIR .","title":"Tensorboard integration"},{"location":"tutorial/notebook/#additional-notebook-parameters","text":"Apart from setting your computing environment, tensorboard, and bucket mounts, we offer additional parameters for you: zone - GCP location zone machine_type - GCP Compute Engine machine type tags - GCP Compute Engine tags to add to the runtime metadata - Custom metadata to apply to this instance service_account - The email address of a service account to use when running the execution instance_owner - Currently supports one owner only. If not specified, all of the service account users of your VM instance\u2019s service account can use the instance. If specified, only the owner will be able to access the notebook. gpu - The hardware GPU accelerator used on this instance. boot_disk - Boot disk configuration to attach to this instance. data_disk - Data disk configuration to attach to this instance. network - The name of the VPC that this instance is in.","title":"Additional notebook parameters"},{"location":"tutorial/notebook/#roles-and-permissions","text":"Permission and suggested roles (applying the principle of least privilege) required for notebook manipulation: WANNA action Permissions Suggested Roles create See full list roles/notebooks.runner , roles/notebooks.admin delete see full list roles/notebooks.admin For accessing the JupyterLab web interface, you must grant the user access to the service account used by the notebooks instance. If the instance owner is set, only this user can access the web interface. Full list of available roles and permission.","title":"Roles and permissions"},{"location":"tutorial/notebook/#local-development-and-ssh","text":"If you wish to develop code in your local IDE and run it on Vertex-AI notebooks, we have a solution for you. Assuming your notebook is already running, you can set up an SSH connection via: wanna notebook ssh --background -n notebook_name Wanna will create an SSH tunnel using GCP IAP from your local environment to your notebook. The --background/-b flag means that the tunnel will be created in the background and you can access the notebook running in GCP at localhost:8080 (port can be customized with --port ). The second possibility is to use --interactive/-i and that will start a bash inside the Compute Engine instance backing your Vertex-AI notebook. Once you set an --background connection to the notebook, you can use your favorite IDE to develop in the notebook. Here we share instructions on how to use VSCode for this.","title":"Local development and SSH"},{"location":"tutorial/notebook/#connecting-with-vscode","text":"Install Jupyter Extension Create a new file with the type Jupyter notebook Select the Jupyter Server: local button in the global Status bar or run the Jupyter: Specify local or remote Jupyter server for connections command from the Command Palette (\u21e7\u2318P). Select option Existing URL and input http://localhost:8080 You should be connected. If you get an error saying something like '_xsrf' argument missing from POST. , it is because the VSCode cannot start a python kernel in GCP. The current workaround is to manually start a kernel at http://localhost:8080 and then in the VSCode connect to the exiting kernel in the right upper corner. A more detailed guide on setting a connection with VSCode to Jupyter can be found at https://code.visualstudio.com/docs/datascience/jupyter-notebooks.","title":"Connecting with VSCode"},{"location":"tutorial/notebook/#example","text":"notebooks: - name: wanna-notebook-trial service_account: instance_owner: machine_type: n1-standard-4 labels: notebook_usecase: wanna-notebook-sample-simple-pip environment: vm_image: framework: pytorch version: 1-9-xla os: debian-10 gpu: count: 1 accelerator_type: NVIDIA_TESLA_V100 install_gpu_driver: true boot_disk: disk_type: pd_standard size_gb: 100 data_disk: disk_type: pd_standard size_gb: 100 bucket_mounts: - bucket_name: us-burger-gcp-poc-mooncloud bucket_dir: data local_path: /home/jupyter/mounted/gcs tensorboard_ref: my-super-tensorboard","title":"Example"},{"location":"tutorial/pipeline/","text":"WANNA ML Pipelines # WANNA ML Pipelines aim at reducing the friction on development cycle to release whilst providing project organization for independent and testable components. It wrapps Kubeflow V2 pipelines with a build and deployment tools for managed GCP VertexAI ML Pipelines service. It has several utils and conventions to reduce boilerplate and speed up development. Tutorial Get started # In this tutorial we will go over several steps that will bootstrap and run a Vertex AI Pipeline(Kubeflow V2) through wanna cli. Setup environment # # Login to GCP gcloud auth login # Auth against GCP docker registries gcloud auth configure-docker europe-west1-docker.pkg.dev # Create python env conda create -n pipeline-tutorial python=3.8 poetry conda activate pipeline-tutorial pip install wanna-ml \\ --index-url https://artifactory.ida.avast.com/artifactory/api/pypi/pypi-remote/simple \\ --extra-index-url https://artifactory.ida.avast.com/artifactory/api/pypi/pypi-local/simple Initialize WANNA project # wanna init --template blank For this turorial we will be using Avast cloud-lab-304213 GCP project as it is available to everyone. Answer the following questions: project_name [project_name]: pipeline_tutorial project_owner_fullname [project owner]: project_owner_email [you@avast.com]: project_version [0.0.0]: project_description [Link to WANNA project page on CML]: project_slug [project_name]: gcp_project_id []: cloud-lab-304213 gcp_service_account []: jacekhebdatest@cloud-lab-304213.iam.gserviceaccount.com gcp_bucket []: wanna-ml-west1-jh Complete installation process cd pipeline_tutorial poetry install Build blank pipeline to check all is well in paradise wanna pipeline build Components # Now that we have a blank bootstraped Kubeflow V2 pipeline we need to add components. In this tutorial we choose to go with self contained components that are independent and testable. There is a recomended structure but it can be tedious to repeat every time and this is where wanna comes in to aliviate this boilerplate. Create first component for data prep # The first component we will create is a data component where we can do some data preparation for the rest of the pipeline wanna components create --output-dir pipeline/components/ the output should be something like component_name [component_name]: data component_author [component author]: Joao Da Silva component_author_email [you@avast.com]: joao.silva1@avast.com component_version [0.0.0]: component_description [Describe your component]: data component for WANNA pipeline tutorial component_url [Link to MLOps project page on CML]: component_slug [data]: component_docker_ref [data]: Select component_framework: 1 - base-cpu 2 - base-cu110 3 - base-cu113 4 - pytorch-xla-1.11 5 - pytorch-gpu-1.10 6 - pytorch-gpu-1.11 7 - sklearn-0.24 8 - tf-cpu.2-6 9 - tf-cpu.2-8 10 - tf-gpu-slim.2-6 11 - tf-gpu-slim.2-8 12 - xgboost-cpu.1-1 Choose from 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 [1]: 1 as a result you can see the following tree structure, which for the most part is a common python lib structure whith the exception of kubeflow's component.yaml tree pipeline/components/data pipeline/components/data \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 component.yaml \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 data \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 data.py \u2514\u2500\u2500 tests \u2514\u2500\u2500 test_data.py 4 directories, 9 files You may also notice that a Dockerfile is present and here is where the python data lib is installed and executed. However we do need to add this Dockerfile to wanna.yaml so that wanna knows about it and can build it and export it. Update wanna docker.images yaml array with images - build_type: local_build_image name: data context_dir: pipeline/components/data/ dockerfile: pipeline/components/data/Dockerfile and add the docker ref data into the pipeline in wanna.yaml so that wanna can link these and expose the docker tag and pipeline compine time. Update wanna.yamll path pipelines[0].docker_image_ref with docker_image_ref: [data] . It should now look like: pipelines: - name: pipeline-tutorial-pipeline schedule: cron: 2 * * * * bucket: gs://wanna-tensorflow-sample-dev pipeline_file: pipeline/pipeline.py pipeline_params: pipeline/params.yaml docker_image_ref: [\"data\"] tensorboard_ref: pipeline-tutorial-board Earlier I mentioned expose the docker tag , this means wanna exports every docker tag from docker.images array as ${NAME_DOCKER_URI} so in this component you can see in the yaml ${DATA_DOCKER_URI} which gets replaced at compile time, this way we have the possibility to have dynamic Kubeflow components versioned according to current pipeline release. Next we may wat to make this component part of our poetry build so that we can run tests, linter and whatnot from a single place. Lets edit our pyproject.toml as follows: [tool.poetry.dependencies] python = \">=3.8,<3.11\" data = {path = \"pipeline/components/data\", develop=true} from poetry >= 1.2 we will be able to just run poetry add pipeline/components/data --editable . Let's install our data lib into the environment and run some poetry lock && poetry install At this point running python -m data.data --help should show: Usage: python -m data.data [OPTIONS] Options: --project TEXT --location TEXT --experiment-name TEXT --help Show this message and exit. as well as a quick test pytest -s pipeline/components/data or go full scale with pre-commit hook git init . && git add . && task build . This will fail on flake8 as expected, as we will be using the var which flake8 complains about. You can now see that you have a self contained component and testable, ready to be added to the kubeflow pipeline. Pipeline # WANNA template creates two files that allow to put together the Kubeflow V2 pipeline that will then be deployed to GCP Vertex AI Pipelines. pipeline/config.py captures wanna compile time exposed environment variables and provides this as configuration to pipeline/pipeline.py . With that in mind let's add our data component to pipeline.py . First we imoprt wanna component loader that will replace ENV vars, namely the container from wanna.components.loader import load_wanna_component Secondly we will load the actual component. wanna_pipeline function should now look like this: @dsl.pipeline( # A name for the pipeline. Use to determine the pipeline Context. name=cfg.PIPELINE_NAME, pipeline_root=cfg.PIPELINE_ROOT ) def wanna_pipeline(eval_acc_threshold: float): pipeline_dir = Path(__file__).parent.resolve() # =================================================================== # Get pipeline result notification # =================================================================== exit_task = ( on_exit() .set_display_name(\"On Exit Dummy Task\") .set_caching_options(False) ) with dsl.ExitHandler(exit_task): load_wanna_component(f\"{pipeline_dir}/components/data/component.yaml\")( experiment_name=cfg.MODEL_DISPLAY_NAME ).set_display_name(\"Data prep\") now with everything in place, lets build the pipeline with wanna pipeline build or with wanna pipeline build --quick if you want to skip docker builds and just verify Kubeflow compiles and components have correct inputs and outputs connected. Running the pipeline in dev mode # When WANNA runs a pipeline from local it will, build & push the component containers to Google Docker registry compile the kubeflow pipeline and upload to gcs the resulting pipeline json spec compile and upload to gcs wanna manifest that allows to run the pipeline from anywhere Trigger the pipeline run and print its dashboard url and running state Assuming we are on \u2714 Compiling pipeline pipeline-tutorial-pipeline succeeded we can now actually run the pipeline. wanna pipeline run --name pipeline-tutorial-pipeline --params pipeline/params.yaml --version dev --sync if all goes with the plan you should see something along the lines(we will be improving the stdout logging overtime): Reading and validating wanna yaml config \u2139 GCP profile 'default' will be used. \u2714 Skipping build for context_dir=pipeline/components/data, dockerfile=pipeline/components/data/Dockerfile and image europe-west1-docker.pkg.dev/cloud-lab-304213/wanna-samples/pipeline_tutorial/data:dev \u2838 Compiling pipeline pipeline-tutorial-pipeline \u2714 Compiling pipeline pipeline-tutorial-pipeline \u2714 Pushing docker image europe-west1-docker.pkg.dev/cloud-lab-304213/wanna-samples/pipeline_tutorial/data:dev \u2139 Uploading wanna running manifest to gs://wanna-ml-west1-jh/pipeline-root/pipeline-tutorial-pipeline/deployment/release/dev/wanna_manifest.json \u2139 Uploading vertex ai pipeline spec to gs://wanna-ml-west1-jh/pipeline-root/pipeline-tutorial-pipeline/deployment/release/dev/pipeline_spec.json \u2714 Pushing pipeline pipeline-tutorial-pipeline \u2826 Running pipeline pipeline-tutorial-pipeline in sync modeCreating PipelineJob \u280b Running pipeline pipeline-tutorial-pipeline in sync modePipelineJob created. Resource name: projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650 To use this PipelineJob in another session: pipeline_job = aiplatform.PipelineJob.get('projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650') View Pipeline Job: https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-pipeline-tutorial-pipeline-20220524103650?project=968728188698 \u2139 Pipeline dashboard at https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-pipeline-tutorial-pipeline-20220524103650?project=968728188698. PipelineJob projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650 current state: PipelineState.PIPELINE_STATE_RUNNING \u2714 Running pipeline pipeline-tutorial-pipeline in sync mode You can clearly see the url to the Vertex AI dashboard where you can inspect the pipeline execution, logs, kubeflow inputs and outputs and any logged metadata. You may have noticed in above the line Uploading wanna running manifest to gs://wanna-cloudlab-europe-west1/wanna-pipelines/wanna-sklearn-sample/deployment/dev/manifests/wanna-manifest.json in the logs. This means wanna publishes has its own pipeline manifest which allow us to run any pipeline version with any set of params. Let's try it: echo \"eval_acc_threshold: 0.79\" > pipeline/params.experiment.yaml wanna pipeline run --manifest gs://wanna-cloudlab-europe-west1/wanna-pipelines/wanna-sklearn-sample/deployment/dev/manifests/wanna-manifest.json --params pipeline/params.experiment.yaml --sync The above snippet will run the pipeline we published earlier with a new set of params. Each manifest version is pushed to gs://${PIPELINE_BUCKET}/pipeline-root/${PIPELINE_NAME}/deployment/release/${VERSION}/wanna_manifest.json so it's easy to trigger these pipelines.","title":"WANNA Pipeline"},{"location":"tutorial/pipeline/#wanna-ml-pipelines","text":"WANNA ML Pipelines aim at reducing the friction on development cycle to release whilst providing project organization for independent and testable components. It wrapps Kubeflow V2 pipelines with a build and deployment tools for managed GCP VertexAI ML Pipelines service. It has several utils and conventions to reduce boilerplate and speed up development.","title":"WANNA ML Pipelines"},{"location":"tutorial/pipeline/#tutorial-get-started","text":"In this tutorial we will go over several steps that will bootstrap and run a Vertex AI Pipeline(Kubeflow V2) through wanna cli.","title":"Tutorial Get started"},{"location":"tutorial/pipeline/#setup-environment","text":"# Login to GCP gcloud auth login # Auth against GCP docker registries gcloud auth configure-docker europe-west1-docker.pkg.dev # Create python env conda create -n pipeline-tutorial python=3.8 poetry conda activate pipeline-tutorial pip install wanna-ml \\ --index-url https://artifactory.ida.avast.com/artifactory/api/pypi/pypi-remote/simple \\ --extra-index-url https://artifactory.ida.avast.com/artifactory/api/pypi/pypi-local/simple","title":"Setup environment"},{"location":"tutorial/pipeline/#initialize-wanna-project","text":"wanna init --template blank For this turorial we will be using Avast cloud-lab-304213 GCP project as it is available to everyone. Answer the following questions: project_name [project_name]: pipeline_tutorial project_owner_fullname [project owner]: project_owner_email [you@avast.com]: project_version [0.0.0]: project_description [Link to WANNA project page on CML]: project_slug [project_name]: gcp_project_id []: cloud-lab-304213 gcp_service_account []: jacekhebdatest@cloud-lab-304213.iam.gserviceaccount.com gcp_bucket []: wanna-ml-west1-jh Complete installation process cd pipeline_tutorial poetry install Build blank pipeline to check all is well in paradise wanna pipeline build","title":"Initialize WANNA project"},{"location":"tutorial/pipeline/#components","text":"Now that we have a blank bootstraped Kubeflow V2 pipeline we need to add components. In this tutorial we choose to go with self contained components that are independent and testable. There is a recomended structure but it can be tedious to repeat every time and this is where wanna comes in to aliviate this boilerplate.","title":"Components"},{"location":"tutorial/pipeline/#create-first-component-for-data-prep","text":"The first component we will create is a data component where we can do some data preparation for the rest of the pipeline wanna components create --output-dir pipeline/components/ the output should be something like component_name [component_name]: data component_author [component author]: Joao Da Silva component_author_email [you@avast.com]: joao.silva1@avast.com component_version [0.0.0]: component_description [Describe your component]: data component for WANNA pipeline tutorial component_url [Link to MLOps project page on CML]: component_slug [data]: component_docker_ref [data]: Select component_framework: 1 - base-cpu 2 - base-cu110 3 - base-cu113 4 - pytorch-xla-1.11 5 - pytorch-gpu-1.10 6 - pytorch-gpu-1.11 7 - sklearn-0.24 8 - tf-cpu.2-6 9 - tf-cpu.2-8 10 - tf-gpu-slim.2-6 11 - tf-gpu-slim.2-8 12 - xgboost-cpu.1-1 Choose from 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 [1]: 1 as a result you can see the following tree structure, which for the most part is a common python lib structure whith the exception of kubeflow's component.yaml tree pipeline/components/data pipeline/components/data \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 component.yaml \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 data \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 data.py \u2514\u2500\u2500 tests \u2514\u2500\u2500 test_data.py 4 directories, 9 files You may also notice that a Dockerfile is present and here is where the python data lib is installed and executed. However we do need to add this Dockerfile to wanna.yaml so that wanna knows about it and can build it and export it. Update wanna docker.images yaml array with images - build_type: local_build_image name: data context_dir: pipeline/components/data/ dockerfile: pipeline/components/data/Dockerfile and add the docker ref data into the pipeline in wanna.yaml so that wanna can link these and expose the docker tag and pipeline compine time. Update wanna.yamll path pipelines[0].docker_image_ref with docker_image_ref: [data] . It should now look like: pipelines: - name: pipeline-tutorial-pipeline schedule: cron: 2 * * * * bucket: gs://wanna-tensorflow-sample-dev pipeline_file: pipeline/pipeline.py pipeline_params: pipeline/params.yaml docker_image_ref: [\"data\"] tensorboard_ref: pipeline-tutorial-board Earlier I mentioned expose the docker tag , this means wanna exports every docker tag from docker.images array as ${NAME_DOCKER_URI} so in this component you can see in the yaml ${DATA_DOCKER_URI} which gets replaced at compile time, this way we have the possibility to have dynamic Kubeflow components versioned according to current pipeline release. Next we may wat to make this component part of our poetry build so that we can run tests, linter and whatnot from a single place. Lets edit our pyproject.toml as follows: [tool.poetry.dependencies] python = \">=3.8,<3.11\" data = {path = \"pipeline/components/data\", develop=true} from poetry >= 1.2 we will be able to just run poetry add pipeline/components/data --editable . Let's install our data lib into the environment and run some poetry lock && poetry install At this point running python -m data.data --help should show: Usage: python -m data.data [OPTIONS] Options: --project TEXT --location TEXT --experiment-name TEXT --help Show this message and exit. as well as a quick test pytest -s pipeline/components/data or go full scale with pre-commit hook git init . && git add . && task build . This will fail on flake8 as expected, as we will be using the var which flake8 complains about. You can now see that you have a self contained component and testable, ready to be added to the kubeflow pipeline.","title":"Create first component for data prep"},{"location":"tutorial/pipeline/#pipeline","text":"WANNA template creates two files that allow to put together the Kubeflow V2 pipeline that will then be deployed to GCP Vertex AI Pipelines. pipeline/config.py captures wanna compile time exposed environment variables and provides this as configuration to pipeline/pipeline.py . With that in mind let's add our data component to pipeline.py . First we imoprt wanna component loader that will replace ENV vars, namely the container from wanna.components.loader import load_wanna_component Secondly we will load the actual component. wanna_pipeline function should now look like this: @dsl.pipeline( # A name for the pipeline. Use to determine the pipeline Context. name=cfg.PIPELINE_NAME, pipeline_root=cfg.PIPELINE_ROOT ) def wanna_pipeline(eval_acc_threshold: float): pipeline_dir = Path(__file__).parent.resolve() # =================================================================== # Get pipeline result notification # =================================================================== exit_task = ( on_exit() .set_display_name(\"On Exit Dummy Task\") .set_caching_options(False) ) with dsl.ExitHandler(exit_task): load_wanna_component(f\"{pipeline_dir}/components/data/component.yaml\")( experiment_name=cfg.MODEL_DISPLAY_NAME ).set_display_name(\"Data prep\") now with everything in place, lets build the pipeline with wanna pipeline build or with wanna pipeline build --quick if you want to skip docker builds and just verify Kubeflow compiles and components have correct inputs and outputs connected.","title":"Pipeline"},{"location":"tutorial/pipeline/#running-the-pipeline-in-dev-mode","text":"When WANNA runs a pipeline from local it will, build & push the component containers to Google Docker registry compile the kubeflow pipeline and upload to gcs the resulting pipeline json spec compile and upload to gcs wanna manifest that allows to run the pipeline from anywhere Trigger the pipeline run and print its dashboard url and running state Assuming we are on \u2714 Compiling pipeline pipeline-tutorial-pipeline succeeded we can now actually run the pipeline. wanna pipeline run --name pipeline-tutorial-pipeline --params pipeline/params.yaml --version dev --sync if all goes with the plan you should see something along the lines(we will be improving the stdout logging overtime): Reading and validating wanna yaml config \u2139 GCP profile 'default' will be used. \u2714 Skipping build for context_dir=pipeline/components/data, dockerfile=pipeline/components/data/Dockerfile and image europe-west1-docker.pkg.dev/cloud-lab-304213/wanna-samples/pipeline_tutorial/data:dev \u2838 Compiling pipeline pipeline-tutorial-pipeline \u2714 Compiling pipeline pipeline-tutorial-pipeline \u2714 Pushing docker image europe-west1-docker.pkg.dev/cloud-lab-304213/wanna-samples/pipeline_tutorial/data:dev \u2139 Uploading wanna running manifest to gs://wanna-ml-west1-jh/pipeline-root/pipeline-tutorial-pipeline/deployment/release/dev/wanna_manifest.json \u2139 Uploading vertex ai pipeline spec to gs://wanna-ml-west1-jh/pipeline-root/pipeline-tutorial-pipeline/deployment/release/dev/pipeline_spec.json \u2714 Pushing pipeline pipeline-tutorial-pipeline \u2826 Running pipeline pipeline-tutorial-pipeline in sync modeCreating PipelineJob \u280b Running pipeline pipeline-tutorial-pipeline in sync modePipelineJob created. Resource name: projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650 To use this PipelineJob in another session: pipeline_job = aiplatform.PipelineJob.get('projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650') View Pipeline Job: https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-pipeline-tutorial-pipeline-20220524103650?project=968728188698 \u2139 Pipeline dashboard at https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/pipeline-pipeline-tutorial-pipeline-20220524103650?project=968728188698. PipelineJob projects/968728188698/locations/europe-west1/pipelineJobs/pipeline-pipeline-tutorial-pipeline-20220524103650 current state: PipelineState.PIPELINE_STATE_RUNNING \u2714 Running pipeline pipeline-tutorial-pipeline in sync mode You can clearly see the url to the Vertex AI dashboard where you can inspect the pipeline execution, logs, kubeflow inputs and outputs and any logged metadata. You may have noticed in above the line Uploading wanna running manifest to gs://wanna-cloudlab-europe-west1/wanna-pipelines/wanna-sklearn-sample/deployment/dev/manifests/wanna-manifest.json in the logs. This means wanna publishes has its own pipeline manifest which allow us to run any pipeline version with any set of params. Let's try it: echo \"eval_acc_threshold: 0.79\" > pipeline/params.experiment.yaml wanna pipeline run --manifest gs://wanna-cloudlab-europe-west1/wanna-pipelines/wanna-sklearn-sample/deployment/dev/manifests/wanna-manifest.json --params pipeline/params.experiment.yaml --sync The above snippet will run the pipeline we published earlier with a new set of params. Each manifest version is pushed to gs://${PIPELINE_BUCKET}/pipeline-root/${PIPELINE_NAME}/deployment/release/${VERSION}/wanna_manifest.json so it's easy to trigger these pipelines.","title":"Running the pipeline in dev mode"},{"location":"tutorial/profile/","text":"WANNA Profile # We make it easy to deploy your resources to multiple environments (e.g., dev/test/prod) with a simple change in CLI flag or with environment variable change. WANNA Profile is a set of parameters that will cascade down to every instance you want to create unless you overwrite them at the instance level. Loading WANNA Profiles # There are two possible ways to load your profile. Include the wanna_profiles section in your main WANNA yaml config with an array of profiles Include the wanna_profiles section in separate profiles.yaml saved wherever on your machine and set env variable WANNA_GCP_PROFILE_PATH=/path/to/your/profiles.yaml Selecting WANNA Profile # Now that the CLI knows about your profiles, you need to select the one you want to use. By default, the profile with the name default is used. You can change that with either WANNA_GCP_PROFILE_NAME=my-profile-name or --profile=my-profile-name . When the selected WANNA Profile is not found, we throw an error. WANNA Profile parameters # wanna_profile section of the yaml config consists of the following inputs: profile_name - name of the WANNA GCP Profile, default will be used if not specified otherwise. You could use also for example dev , prod , or any other string. project_id - GCP project id. zone - (optional) GCP location zone. region - (optional) GCP location region. If the zone is set and the region not, we automatically parse the region from the zone (e.g., zone us-east1-c automatically sets region us-east1 if the region is not supplied by the user). labels - (optional) GCP resource labels that will be added to all GCP resources you create with this profile. By default, we also add a few labels based on wanna_project section. bucket - (optional) GCS Bucket that can later be used in uploading manifests, storing logs, and so on depending on the resource type. service_account - (optional) GCP service account that will be used by the created resources. If not specified, usually the default service account for each resource type is used. Example use case # gcp_profiles: - profile_name: default project_id: gcp-test-project zone: europe-west1-b bucket: wanna-ml-test labels: - env: test - profile_name: prod project_id: gcp-prod-project zone: europe-west4-a bucket: wanna-ml-prod labels: - env: prod Now the command wanna ... will use the information from the default profile and deploy to europe-west1-b . When you are ready with your testing, you can call wanna ... --profile=prod to deploy to the production GCP project and zone europe-west4-a .","title":"WANNA Profile"},{"location":"tutorial/profile/#wanna-profile","text":"We make it easy to deploy your resources to multiple environments (e.g., dev/test/prod) with a simple change in CLI flag or with environment variable change. WANNA Profile is a set of parameters that will cascade down to every instance you want to create unless you overwrite them at the instance level.","title":"WANNA Profile"},{"location":"tutorial/profile/#loading-wanna-profiles","text":"There are two possible ways to load your profile. Include the wanna_profiles section in your main WANNA yaml config with an array of profiles Include the wanna_profiles section in separate profiles.yaml saved wherever on your machine and set env variable WANNA_GCP_PROFILE_PATH=/path/to/your/profiles.yaml","title":"Loading WANNA Profiles"},{"location":"tutorial/profile/#selecting-wanna-profile","text":"Now that the CLI knows about your profiles, you need to select the one you want to use. By default, the profile with the name default is used. You can change that with either WANNA_GCP_PROFILE_NAME=my-profile-name or --profile=my-profile-name . When the selected WANNA Profile is not found, we throw an error.","title":"Selecting WANNA Profile"},{"location":"tutorial/profile/#wanna-profile-parameters","text":"wanna_profile section of the yaml config consists of the following inputs: profile_name - name of the WANNA GCP Profile, default will be used if not specified otherwise. You could use also for example dev , prod , or any other string. project_id - GCP project id. zone - (optional) GCP location zone. region - (optional) GCP location region. If the zone is set and the region not, we automatically parse the region from the zone (e.g., zone us-east1-c automatically sets region us-east1 if the region is not supplied by the user). labels - (optional) GCP resource labels that will be added to all GCP resources you create with this profile. By default, we also add a few labels based on wanna_project section. bucket - (optional) GCS Bucket that can later be used in uploading manifests, storing logs, and so on depending on the resource type. service_account - (optional) GCP service account that will be used by the created resources. If not specified, usually the default service account for each resource type is used.","title":"WANNA Profile parameters"},{"location":"tutorial/profile/#example-use-case","text":"gcp_profiles: - profile_name: default project_id: gcp-test-project zone: europe-west1-b bucket: wanna-ml-test labels: - env: test - profile_name: prod project_id: gcp-prod-project zone: europe-west4-a bucket: wanna-ml-prod labels: - env: prod Now the command wanna ... will use the information from the default profile and deploy to europe-west1-b . When you are ready with your testing, you can call wanna ... --profile=prod to deploy to the production GCP project and zone europe-west4-a .","title":"Example use case"},{"location":"tutorial/project/","text":"WANNA project # WANNA project settings set some basic values about your project. wanna_project section of the yaml config consists of the following inputs: name - the name of the wanna project should be unique, this name will be used in the docker service for naming docker images and in labeling GCP resources. Hence it can be used also for budget monitoring. version - Currently used only in labeling GCP resources, we expect to introduce new API versions and then this parameter will gain more importance. authors - List of email addresses, currently used only in GCP resource labeling but soon also in monitoring. Example # wanna_project: name: wanna-julia-notebooks version: 1 authors: [harry.potter@avast.com, ronald.weasley@avast.com]","title":"WANNA Project"},{"location":"tutorial/project/#wanna-project","text":"WANNA project settings set some basic values about your project. wanna_project section of the yaml config consists of the following inputs: name - the name of the wanna project should be unique, this name will be used in the docker service for naming docker images and in labeling GCP resources. Hence it can be used also for budget monitoring. version - Currently used only in labeling GCP resources, we expect to introduce new API versions and then this parameter will gain more importance. authors - List of email addresses, currently used only in GCP resource labeling but soon also in monitoring.","title":"WANNA project"},{"location":"tutorial/project/#example","text":"wanna_project: name: wanna-julia-notebooks version: 1 authors: [harry.potter@avast.com, ronald.weasley@avast.com]","title":"Example"},{"location":"tutorial/tensorboard/","text":"WANNA Tensorboard # Many GCP services can work directly with Tensorboards. For that reason we offer you a simple solution on how to include them in your resources. Tensorboards can either be used as a separate resource with wanna tensorboard create/list/delete or you can use them similarly to docker images as a dependency of other resources with tensorboard_ref key. In the second case, Tensorboards are automatically created when needed and used when already existing. Tensorboard parameters # Tensorboards take only two parameters: name - name of the tensorboard, not to confuse with resource name (name= my-tensorboard , resource name= projects/{project}/locations/{location}/tensorboards/{tensorboard_id}/ ) region - GCP location Example # tensorboards: - name: wanna-sample-dashboard jobs: - name: custom-training-job-with-python-package region: europe-west1 worker: python_package: docker_image_ref: tensorflow package_gcs_uri: \"gs://wanna-ml/trainer-0.1.tar.gz\" module_name: \"trainer.task\" args: ['--epochs=100', '--steps=100', '--distribute=single'] gpu: accelerator_type: NVIDIA_TESLA_V100 count: 1 tensorboard_ref: my-nice-tensorboard Integration with other services # Integration with tensorboard depends on the resource. For example Custom Jobs pass the path to the tensorboard in env var AIP_TENSORBOARD_LOG_DIR . When using Keras for training, the integration in your code could look like this: from tensorflow.keras.callbacks import TensorBoard # Define Tensorboard as a Keras callback tensorboard = TensorBoard( log_dir=os.getenv(\"AIP_TENSORBOARD_LOG_DIR\"), histogram_freq=1, write_images=True ) keras_callbacks = [tensorboard] model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps, callbacks=keras_callbacks) Check the job samples for a complete example. With notebooks, you will need tb-gcp-uploader as specified here . We also export the link to the tensorboard directory as AIP_TENSORBOARD_LOG_DIR . But you will need to handle the log export yourself. Roles and permissions # Permission and suggested roles (applying the principle of least privilege) required for tensorboard manipulation: WANNA action Permissions Suggested Roles create aiplatform.tensorboards.create , aiplatform.tensorboards.list roles/aiplatform.user delete aiplatform.tensorboards.delete , aiplatform.tensorboards.list roles/aiplatform.user list aiplatform.tensorboards.list , aiplatform.tensorboardExperiments.* , aiplatform.tensorboardRuns.* roles/aiplatform.viewer , roles/aiplatform.user access the dashboard aiplatform.tensorboards.recordAccess roles/aiplatform.tensorboardWebAppUser Full list of available roles and permission.","title":"WANNA Tensorboard"},{"location":"tutorial/tensorboard/#wanna-tensorboard","text":"Many GCP services can work directly with Tensorboards. For that reason we offer you a simple solution on how to include them in your resources. Tensorboards can either be used as a separate resource with wanna tensorboard create/list/delete or you can use them similarly to docker images as a dependency of other resources with tensorboard_ref key. In the second case, Tensorboards are automatically created when needed and used when already existing.","title":"WANNA Tensorboard"},{"location":"tutorial/tensorboard/#tensorboard-parameters","text":"Tensorboards take only two parameters: name - name of the tensorboard, not to confuse with resource name (name= my-tensorboard , resource name= projects/{project}/locations/{location}/tensorboards/{tensorboard_id}/ ) region - GCP location","title":"Tensorboard parameters"},{"location":"tutorial/tensorboard/#example","text":"tensorboards: - name: wanna-sample-dashboard jobs: - name: custom-training-job-with-python-package region: europe-west1 worker: python_package: docker_image_ref: tensorflow package_gcs_uri: \"gs://wanna-ml/trainer-0.1.tar.gz\" module_name: \"trainer.task\" args: ['--epochs=100', '--steps=100', '--distribute=single'] gpu: accelerator_type: NVIDIA_TESLA_V100 count: 1 tensorboard_ref: my-nice-tensorboard","title":"Example"},{"location":"tutorial/tensorboard/#integration-with-other-services","text":"Integration with tensorboard depends on the resource. For example Custom Jobs pass the path to the tensorboard in env var AIP_TENSORBOARD_LOG_DIR . When using Keras for training, the integration in your code could look like this: from tensorflow.keras.callbacks import TensorBoard # Define Tensorboard as a Keras callback tensorboard = TensorBoard( log_dir=os.getenv(\"AIP_TENSORBOARD_LOG_DIR\"), histogram_freq=1, write_images=True ) keras_callbacks = [tensorboard] model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps, callbacks=keras_callbacks) Check the job samples for a complete example. With notebooks, you will need tb-gcp-uploader as specified here . We also export the link to the tensorboard directory as AIP_TENSORBOARD_LOG_DIR . But you will need to handle the log export yourself.","title":"Integration with other services"},{"location":"tutorial/tensorboard/#roles-and-permissions","text":"Permission and suggested roles (applying the principle of least privilege) required for tensorboard manipulation: WANNA action Permissions Suggested Roles create aiplatform.tensorboards.create , aiplatform.tensorboards.list roles/aiplatform.user delete aiplatform.tensorboards.delete , aiplatform.tensorboards.list roles/aiplatform.user list aiplatform.tensorboards.list , aiplatform.tensorboardExperiments.* , aiplatform.tensorboardRuns.* roles/aiplatform.viewer , roles/aiplatform.user access the dashboard aiplatform.tensorboards.recordAccess roles/aiplatform.tensorboardWebAppUser Full list of available roles and permission.","title":"Roles and permissions"}]}